---
title: "Real-time PAA Disinfection Control in Municipal Wastewater Treament"
author: "Kate Newhart"
date: "11/13/2019"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	fig.path = "figures/",
	fig.width = 6.5,
	dpi = 300
)
```

# October 2018 Evaluation
Need inf flow to calculate HRT
```{r Import Oct 2018 data, include=FALSE}
library(xts)
##### PAA Grab Data #####
oct.paa <- readxl::read_excel("data/PAA PROFILE DATA_08-12-18.xlsx", 
                              sheet = "Oct 2 to 15, 2018", range = "A1:V170")[-1,]
n.datetime <- which(colnames(oct.paa) == "Date and Time")
oct.paa.index <- oct.paa[,n.datetime]
oct.paa <- sapply(oct.paa[,-n.datetime], function(x) as.numeric(x))
colnames(oct.paa) <- stringr::str_replace_all(colnames(oct.paa), c(" " = "." , "-" = "" ))
oct.paa <- xts(oct.paa, order.by = oct.paa.index[[1]])

###### North Carbovis Data #####
vis.data <- readxl::read_excel("data/NNE Carbovis Data 2018.xlsx",
                               sheet = "Inst DL Data", col_types = c("date",
                                                                     "text", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip"), skip = 6)
vis.data <- vis.data[which(vis.data[,2] == "Valid"),-2]
colnames(vis.data) <- c("Time", "CODto (mg/L)", "CODto (V)",
                        "TSS (mg/L)", "TSS (V)",
                        "UVT (%)", "UVT (V)",
                        "CODds (mg/L)", "CODds (V)",
                        "SACto (1/m)", "SACto (V)")
vis.data <- xts(vis.data[,-1], order.by = as.POSIXct(as.data.frame(vis.data[,1])[,1], format = "%Y-%m-%d %H-%M-%S"))

##### North Secondary - Online #####
nsec.online <- as.data.frame(suppressWarnings(readxl::read_excel("data/North Secondary and Disinfection Process Data_2018.xlsx", sheet = "NSEC Online Data", col_names = FALSE,
                                                                 col_types = c("date", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric"), 
                                                                 skip = 4)))
nsec.online <- xts(nsec.online[,-1], order.by = nsec.online[,1])
colnames(nsec.online) <- c("NSEC Influent Flow", "NSEC Influent Temp","NSEC Influent NH3","NSEC Influent TSS","NSEC Influent COD",
                           "NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3",
                           "GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow",
                           "AB-10 Influent Flow","AB-10 A-Pass Temp","AB-10 A-Pass pH","AB-10 A-Pass DO","AB-10 A-Pass NH3","AB-10 A-Pass NO3","AB-10 B-Pass DO","AB-10 C-Pass pH	AB-10","C-Pass DO","AB-10 C-Pass NH3","AB-10 C-Pass NO3","AB-10 MLSS","AB-10 MLR Flow","Quad 4 RAS Flow","Quad 4 Basins in Service","AB-10 RAS Flow","NSEC Aerobic SRT",
                           "NSEC Effluent NH3","NSEC Effluent NO3","NSEC Effluent OP","NSEC Effluent TSS","NSEC Effluent NO5","NSEC Effluent Flow")
cols2remove <- c("NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3","GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow")
nsec.online <- nsec.online[,-sapply(cols2remove, function(x) which(colnames(nsec.online) == x))]
```

```{r Merge Oct 2018 data, include=FALSE}
mergeData <- function(list.x, sort.by = 1, average = FALSE) {
  require(xts)
  if (exists("new.data")) rm(new.data)
  all.data <- do.call(merge, list.x)
  all.data.index <- which(!is.na(all.data[,sort.by]))
  for(i in 1:(length(all.data.index)-1)) {
    row.start <- all.data.index[i]
    row.stop <- all.data.index[i+1]
    if((row.stop-row.start) == 1) {
      next
    }
    if(!average) data.locf <- na.locf(all.data[(row.start+1):row.stop,])
    if(average) {
      data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
      rownames(data.avg) <- as.character(index(all.data)[row.stop])
      
    }
    if (!exists("new.data")) {
      if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
      if(average) new.data <- data.avg
    }
    if (exists("new.data")) {
      if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
      if(average) new.data <- rbind(new.data, data.avg)
    }
  }
  new.data <- na.omit(new.data)
  na.fix <- which(!is.na(as.POSIXct(rownames(new.data), format = "%Y-%m-%d %H:%M:%S")))
  new.data.xts <- xts(new.data[na.fix,], order.by = as.POSIXct(rownames(new.data)[na.fix], format = "%Y-%m-%d %H:%M:%S"))
  
  return(new.data.xts)
}


all.data <- oct.paa
r <- paste0(range(index(all.data))[1],"/",range(index(all.data))[2])
all.data <- mergeData(list.x = list(all.data,vis.data[r],nsec.online[r,22:28]))
all.data <- na.omit(all.data)
remove.cols.names <- c("Initial.PAA.Demand.or.Decay", "DPAA.Samples","Sample.Time..1_min.","Detention.Time","PAA.Pump.Total.Flow", "PAA.Set.Point.Dose.Algorithm", "...10" , "Volume.to.1.min..Sample" , "Time.to.1.min..Sample", "Total.Basin.Volume", "DT.of.1.2.Basin", "SPBased.Disinfection.CT",     "CalcBased.Disinfection.CT", "CODto..V.", "CODds..V.", "TSS..V.", "UVT..V.", "UVT..V.", "SACto..V.")
remove.cols <- sapply(remove.cols.names, function(x) which(colnames(all.data) == x))
all.data <- all.data[,-remove.cols]
```


NN Models were built to predict PAA at the "1-min" and "1/2 basin" sample point to predict C1 and C2
```{r include=FALSE}
Optimize <- FALSE
percent.train = 0.8
if(Optimize){
  predict.col.name <- c("PAA...1.min..Sample", "PAA...1.2.Basin.Sampling")
training.index = NULL
all.data <- all.data[,which(colnames(all.data) != predict.col.name[2])]
predict.col.name <- predict.col.name[1]
iterations <- 10 

# NNopt <- function(all.data, predict.col.name, percent.train = 0.8, training.index=NULL) {
  library(doSNOW)
  library(parallel)
  library(neuralnet)

  ## Prep data
  predict.column <- which(colnames(all.data) == predict.col.name)
  all.data <- na.omit(data.frame(all.data))
  
  ## Scale data using min-max method
  max <- apply(all.data, 2, max)
  min <- apply(all.data, 2, min)
  scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))
  
  for(iteration in 1:iterations) {
  ## Create training and test datasets
  # if(is.null(training.index)) {
    set.seed(Sys.time())
    training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
  # }
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  
  ## Find optimum number of variables
  n <- which(colnames(all.data) != predict.col.name)
  x.combos <- c(3,4,5,6,7,8,9)
  for(no.vars in x.combos) {

    x <- data.frame(combn(n,no.vars))
    x.index <- seq(1:ncol(x))
    
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    results <- foreach::foreach(i = x.index, .combine = rbind) %dopar% {
      
      fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                                paste(colnames(all.data)[x[,i]], collapse= "+")))
      nodes <- round(no.vars/2)
      NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
      
      ## Predict using NN
      predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
      if(is.null(predict.NN)){
        RMSE.NN <- NA
        r2.NN <- NA
      } else {
        predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
        # Calculate Root Mean Square Error (RMSE)
        RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
        # Calculate R-sq
        r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
      }
      data.frame("Fmla" = as.character(fmla[3]),
                 "RMSE" = RMSE.NN,
                 "r2" = as.numeric(r2.NN),
                 "Nodes" = nodes)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
    write.csv(results, paste0("results/PAA_1min_NN_",results[1,4],"nodes_",no.vars,"vars_",iteration,".csv"))
    
    
  }
  }
  
  file <- paste0("results/",list.files(path="results"))
  for(i in 1:length(file)){
    raw <- read.csv(file[i], row.names=1, stringsAsFactors=FALSE)
    if(i ==1) data <- raw
    if(i != 1)data <- rbind(data,raw)
  }
  
  
  
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    data.avg <- foreach::foreach(i = 1:length(unique(data$Fmla)), .combine = rbind) %dopar% {
  # for(i in 1:length(unique(data$Fmla))){
  # for(i in 4594:length(unique(data$Fmla))){
    # if(i == 1) data.avg <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    # if(i != 1) data.avg <- c(data.avg, mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])]))
    result <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    data.frame("Fmla" = unique(data$Fmla)[i],
               "R2" = result)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
}

# data.avg <- sapply(unique(data$Fmla), function(x) mean(data$r2[which(data$Fmla == x)]))
# data.avg <- cbind(unique(data$Fmla), sapply(data.index, function(x) mean(data$Fmla[x])))

predict.col.name <- c("PAA...1.min..Sample")
## Prep data
predict.column <- which(colnames(all.data) == predict.col.name)
all.data <- na.omit(data.frame(all.data))
## Scale data using min-max method
max <- apply(all.data, 2, max)
min <- apply(all.data, 2, min)
scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))

# for(obs in c(10, 30,35,40,45,50,55,60,65,70,90,110)) {
# for (i in 1:(nrow(all.data)*(1-percent.train))) {
library(doSNOW)
library(parallel)
library(neuralnet)

# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
results <- foreach::foreach(i = 1:(nrow(all.data)*(1-percent.train)), .combine = rbind) %dopar% {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  # fmla <- as.formula("PAA...1.min..Sample ~ Pump.Flow.Based.PAA.Dose + Temp..of.NSEC.Main.Ch. + Temp..of.the.Atmos. + Secondary.Effluent.Flow + NSEC.Effluent.OP + NSEC.Effluent.NO5 + NSEC.Effluent.Flow")
  fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                     paste(colnames(all.data)[-c(predict.column,predict.column+1)], collapse= "+")))
  
  nodes <- 3
  NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
  predict.NN <-  neuralnet::compute(NN, testing.NN[i,])
  predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])

  # t <- 122.2683*testing.data$Secondary.Effluent.Flow[i]^-0.9312

  return(data.frame(testing.data[i,predict.column],
                      predict.NN))
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
rownames(results) <- rownames(all.data)[-c(1:ceiling(nrow(all.data)*(percent.train)))]
colnames(results) <- c("Actual", "NN C")
results.metrics <- (sum((results[,1] - results[,2])^2) / nrow(results)) ^ 0.5 # RMSE
```


```{r Analyze OCt 2018 data, eval=FALSE, include=FALSE}
for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  C <- c(C1[i,2], C3[i,2]) # NN predicted C
  t <- c(C1[i,3], C3[i,3]) # HRT
  exponential.model <- lm(log(C) ~ t) # Exponential model fit of NN predictions
  k <- -1*exponential.model$coefficients[2]
  C0 <- exp(exponential.model$coefficients[1])
  t_f <- 2775*testing.data$N..Basin.Outfall[i]^-.959
  Ct <- C0/k-C0/k*exp(-k*t_f) # Ct under exponential curve to end of basin
  
  if(i == 1) results <- Ct
  if(i > 1) results <- rbind(results, Ct)
  # 
  # C <- c(training.data$PAA...1.min..Sample, training.data$PAA...1.2.Basin.Sampling)
  # t <- c(130.5293*training.data$N..Basin.Outfall^-0.9495, 
  #        1477.2*training.data$N..Basin.Outfall^-.958)
  # exponential.model <- lm(log(C) ~ t)
  # k <- -1*exponential.model$coefficients[2]
  # C0 <- exp(exponential.model$coefficients[1])
  # Ct <- C0/k-C0/k*exp(-k*t) # Ct under single exponential curve
}

Ct.predicted <- results

for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  C <- c(C1[i,1], C3[i,1]) # Actual C
  t <- c(C1[i,3], C3[i,3]) # HRT
  exponential.model <- lm(log(C) ~ t) # Exponential model fit of NN predictions
  k <- -1*exponential.model$coefficients[2]
  C0 <- exp(exponential.model$coefficients[1])
  t_f <- 2775*testing.data$N..Basin.Outfall[i]^-.959
  Ct <- C0/k-C0/k*exp(-k*t_f) # Ct under exponential curve to end of basin
  
  if(i == 1) results <- Ct
  if(i > 1) results <- rbind(results, Ct)
}

Ct.actual <- results
r2.NN <- cor(Ct.actual, Ct.predicted)^2


n.obs <- as.numeric(nrow(all.data) - nrow(Ct.actual))
png(paste0("Ct_",n.obs,"obs.png"), width = 600, height = 400)
plot(Ct.actual, type="l", lwd=1.5,xlab = "Sampling Event", ylab="Ct (mg/L*min)", main=paste0("Ct - trained on ",n.obs," obs"))
points(Ct.predicted, type="l", col = "purple", lwd=1.5)
legend("topleft", legend = c("Actual Ct", "Predicted Ct"), lty = 1, lwd=1.5,col=c("black", "purple"))
text(x=nrow(Ct.actual),y=min(Ct.actual), labels=paste("R2 =", as.character(round(r2.NN,2))), adj=1)
dev.off()

print(paste("For",as.character(n.obs),
  "obs, R2 =", as.character(round(r2.NN,2))))
# }
```



