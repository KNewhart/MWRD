---
title: "MWRD_PAA_2019-05-24"
---
# Import data and functions
```{r}
rm(list = ls())
# setwd("..")
# source("functions/r-sq.R")
r.sq <- function(raw,fit) {
  # y <- raw
  # y_bar <- mean(raw)
  # y_hat <- fit
  # r_sq <- sum((y_hat-y_bar)^2) / sum((y-y_bar)^2)
  r_sq <- cor(raw,fit)^2
  return(r_sq)
}
# setwd("GitHub")
# setwd("~/KNewhart/MWRD")
library(xts)


### FAIL ###
rbindNN <- function(i.range, i.function, ...) {
    library(doSNOW)
    library(parallel)
    library(neuralnet)
    
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    results <- foreach::foreach(i = i.range, .combine = rbind) %dopar% {
      data.frame(i.function(i,...))
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    return(results)
}

mergeData <- function(list.x, sort.by = 1, average = FALSE) {
  library(xts)
  # library(doSNOW)
  # library(parallel)
  # library(neuralnet)
  
  all.data <- do.call(merge, list.x)
  all.data.index <- which(!is.na(all.data[,sort.by]))

  ### FAIL ###
  # loop <- function(i, ...) {
  #   all.data <- get("all.data", envir = .GlobalEnv)
  #   all.data.index <- get("all.data.index", envir = .GlobalEnv)
  #   row.start <- all.data.index[i]
  #   row.stop <- all.data.index[i+1]
  #   if((row.stop-row.start) == 1) {
  #     next
  #   }
  #   if(!average) data.locf <- na.locf(all.data[(row.start+1):row.stop,])
  #   if(average) {
  #     data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
  #     rownames(data.avg) <- as.character(index(all.data)[row.stop])
  #     
  #   }
  #   if (!exists("new.data")) {
  #     if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
  #     if(average) new.data <- data.avg
  #   }
  #   if (exists("new.data")) {
  #     if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
  #     if(average) new.data <- rbind(new.data, data.avg)
  #   }
  #   
  # }
  
  # new.data <- rbindNN(i.range = 1:(length(all.data.index)-1), i.function = loop, all.data, all.data.index) # FAIL
   # 
   # # detect cores with parallel() package
   #  nCores <- detectCores(logical = FALSE)
   #  # detect threads with parallel()
   #  nThreads<- detectCores(logical = TRUE)
   #  
   #  # Create doSNOW compute cluster
   #  cluster = makeCluster(nThreads, type = "SOCK")
   #  class(cluster);
   #  
   #  # register the cluster
   #  registerDoSNOW(cluster)
    
    # results <- foreach::foreach(i = 1:(length(all.data.index)-1), .combine = rbind) %dopar% {
      
  for(i in 1:(length(all.data.index)-1)) {
    row.start <- all.data.index[i]
    row.stop <- all.data.index[i+1]
    if((row.stop-row.start) == 1) {
    } else {
          if(!average) data.locf <- na.locf(all.data[(row.start+1):row.stop,])
    if(average) {
      data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
      rownames(data.avg) <- as.character(index(all.data)[row.stop])

    }
    if (!exists("new.data")) {
      if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
      if(average) new.data <- data.avg
    }
    if (exists("new.data")) {
      if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
      if(average) new.data <- rbind(new.data, data.avg)
    }
    
    }

    }
    
    #  # stop cluster and remove clients
    # stopCluster(cluster)
    # 
    # # insert serial backend, otherwise error in repetetive tasks
    # registerDoSEQ()
    # 
    # # clean up a bit.
    # invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
  
  new.data <- na.omit(new.data)
  na.fix <- which(!is.na(as.POSIXct(rownames(new.data), format = "%Y-%m-%d %H:%M:%S")))
  new.data.xts <- xts(new.data[na.fix,], order.by = as.POSIXct(rownames(new.data)[na.fix], format = "%Y-%m-%d %H:%M:%S"))
  
  return(new.data.xts)
}





##### PAA Process Data - Grab #####
delta <- intToUtf8(0x0394)
# Daily data
process.data <- readxl::read_excel("data/Copy of PAA Process Data Clean KN.xlsx", 
                                   sheet = "Process Data", skip = 1)
process.data <- process.data[-1,]
n.paa.grab <- xts::xts(apply(process.data[,c(12:17,19:27)], 2, function(x) as.numeric(x)), order.by =  as.POSIXct(as.data.frame(process.data[,18])[,1], format = "%Y-%m-%d %H:%M:%S"))
colnames(n.paa.grab) <- c("PAA Dosing Pump Total Flow (gpm)", #1 
                          "PAA Dose (mg/L)", #2
                          "PAA Setpoint (mg/L)", #3 
                          "Upstream  Residual (mg/L)", #4 
                          # paste0(delta,"PAA (mg/L)"),	#5
                          "deltaPAA (mg/L)", #5
                          "Pre-Disinfection E. coli (MPN/100 mL)",  #6
                          "Effluent Discharge (MGD)", #7
                          "Contact Tank Volume (MG)", #8
                          "Detention Time (min)", #9
                          "Time to Upstream Sample Point (min)", #10
                          "Log Removal (N0/N)", #11
                          "Effluent E. coli (MPN/100 mL)", #12
                          "CT (mg/L*min)", #13
                          "CuT (mg/L*min)", #14
                          "Ambient Temperature")#15
colnames(n.paa.grab) <- stringr::str_replace_all(colnames(n.paa.grab), c(" " = ".", "/" = "." , "-" = "","[(]" = "", "[)]" = "", "[*]"="."))
rm(process.data)


##### October PAA Process Data - Grab #####
oct.paa <- readxl::read_excel("data/Copy of PAA Process Data Clean KN.xlsx", 
                              sheet = "Oct 2 to 15, 2018", range = "A1:V170")[-1,]
n.datetime <- which(colnames(oct.paa) == "Date and Time")
oct.paa.index <- oct.paa[,n.datetime]
oct.paa <- sapply(oct.paa[,-n.datetime], function(x) as.numeric(x))
colnames(oct.paa) <- stringr::str_replace_all(colnames(oct.paa), c(" " = "." , "-" = "" ))
oct.paa <- xts(oct.paa, order.by = oct.paa.index[[1]])

###### North Carbovis Data #####
vis.data <- readxl::read_excel("data/NNE Carbovis Data2.xlsx",
                               sheet = "Inst DL Data", col_types = c("date",
                                                                     "text", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip"), skip = 6)
vis.data <- vis.data[which(vis.data[,2] == "Valid"),-2]
colnames(vis.data) <- c("Time", "CODto (mg/L)", "CODto (V)",
                        "TSS (mg/L)", "TSS (V)",
                        "UVT (%)", "UVT (V)",
                        "CODds (mg/L)", "CODds (V)",
                        "SACto (1/m)", "SACto (V)")
vis.data <- xts(vis.data[,-1], order.by = as.POSIXct(as.data.frame(vis.data[,1])[,1], format = "%Y-%m-%d %H-%M-%S"))


##### North Secondary - Online #####
## North secondary online
nsec.online <- as.data.frame(suppressWarnings(readxl::read_excel("data/North Secondary and Disinfection Process Data_20190215.xlsx", sheet = "NSEC Online Data", col_names = FALSE,
                                                                 col_types = c("date", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric"), 
                                                                 skip = 4)))
nsec.online <- xts(nsec.online[,-1], order.by = nsec.online[,1])
colnames(nsec.online) <- c("NSEC Influent Flow", "NSEC Influent Temp","NSEC Influent NH3","NSEC Influent TSS","NSEC Influent COD",
                           "NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3",
                           "GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow",
                           "AB-10 Influent Flow","AB-10 A-Pass Temp","AB-10 A-Pass pH","AB-10 A-Pass DO","AB-10 A-Pass NH3","AB-10 A-Pass NO3","AB-10 B-Pass DO","AB-10 C-Pass pH	AB-10","C-Pass DO","AB-10 C-Pass NH3","AB-10 C-Pass NO3","AB-10 MLSS","AB-10 MLR Flow","Quad 4 RAS Flow","Quad 4 Basins in Service","AB-10 RAS Flow","NSEC Aerobic SRT",
                           "NSEC Effluent NH3","NSEC Effluent NO3","NSEC Effluent OP","NSEC Effluent TSS","NSEC Effluent NO5","NSEC Effluent Flow")


# nsec.online <- nsec.online["2018-11-04/2018-12-01"]
cols2remove <- c("NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3","GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow")

nsec.online <- nsec.online[,-sapply(cols2remove, function(x) which(colnames(nsec.online) == x))]

more.nsec <- readxl::read_excel("data/PAA-Ecoli.xlsx", 
                                sheet = "Sheet1", col_types = c("date", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric"))
more.nsec <- xts(more.nsec[,-1], order.by = more.nsec[,1][[1]])
colnames(more.nsec) <- c("PAA Upstream Residual", "PAA Total Flow", "Dis North Flow", 
                         "Temperature NSEC Inf", "ASRT", "NSEC Effluent NH3",
                         "NSEC Effluent NO3","NSEC Effluent OP","NSEC Effluent TSS",
                         "NSEC Effluent NO5","NSEC Effluent Flow")

library(readr)
more.vis <- read_csv("data/Carbovis_May2019.csv", 
    col_types = cols(Time = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_1 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_2 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_3 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_4 = col_datetime(format = "%m/%d/%Y %H:%M")))
r <- range(data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1])
# more.vis.all <- mergeData(list.x=list(xts(na.omit(more.vis[,2]), order.by = data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1]),
                                  # xts(na.omit(more.vis[,4]), order.by = data.frame(more.vis[-which(is.na(more.vis[,4])),3][1])[,1])[paste0(r[1],"/",r[2])]))
more.vis.all <- merge(xts(na.omit(more.vis[,2]), order.by = data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1]),
                                  xts(na.omit(more.vis[,4]), order.by = data.frame(more.vis[-which(is.na(more.vis[,4])),3][1])[,1])[paste0(r[1],"/",r[2])])
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,6]), order.by = data.frame(more.vis[-which(is.na(more.vis[,6])),5][1])[,1]))
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,8]), order.by = data.frame(more.vis[-which(is.na(more.vis[,8])),7][1])[,1]))
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,10]), order.by = data.frame(more.vis[-which(is.na(more.vis[,10])),9][1])[,1]))
more.vis.all <- more.vis.all[!is.na(more.vis.all[,1]),]

all.data <- oct.paa
r <- paste0(range(index(all.data))[1],"/",range(index(all.data))[2])
all.data <- mergeData(list.x = list(all.data,vis.data[r]))
all.data <- mergeData(list.x = list(all.data, nsec.online[r,22:28]), average = FALSE)
all.data <- na.omit(all.data)
remove.cols.names <- c("Initial.PAA.Demand.or.Decay", "DPAA.Samples","Sample.Time..1_min.","Detention.Time","PAA.Pump.Total.Flow", "PAA.Set.Point.Dose.Algorithm", "...10" , "Volume.to.1.min..Sample" , "Time.to.1.min..Sample", "Total.Basin.Volume", "DT.of.1.2.Basin", "SPBased.Disinfection.CT",     "CalcBased.Disinfection.CT", "CODto..V.", "CODds..V.", "TSS..V.", "UVT..V.", "UVT..V.", "SACto..V.")
remove.cols <- sapply(remove.cols.names, function(x) which(colnames(all.data) == x))
all.data <- all.data[,-remove.cols]


```

# NN Models were built to predict PAA at the "1-min" sample point to predict C1
```{r include=FALSE}
Optimize <- FALSE
if(Optimize){
  predict.col.name <- c("PAA...1.min..Sample", "PAA...1.2.Basin.Sampling")

percent.train = 0.8
training.index = NULL
all.data <- all.data[,which(colnames(all.data) != predict.col.name[2])]
predict.col.name <- predict.col.name[1]
iterations <- 10 

# NNopt <- function(all.data, predict.col.name, percent.train = 0.8, training.index=NULL) {
  library(doSNOW)
  library(parallel)
  library(neuralnet)

  ## Prep data
  predict.column <- which(colnames(all.data) == predict.col.name)
  all.data <- na.omit(data.frame(all.data))
  
  ## Scale data using min-max method
  max <- apply(all.data, 2, max)
  min <- apply(all.data, 2, min)
  scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))
  
  for(iteration in 1:iterations) {
  ## Create training and test datasets
  # if(is.null(training.index)) {
    set.seed(Sys.time())
    training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
  # }
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  
  ## Find optimum number of variables
  n <- which(colnames(all.data) != predict.col.name)
  x.combos <- c(3,4,5,6,7,8,9)
  for(no.vars in x.combos) {

    x <- data.frame(combn(n,no.vars))
    x.index <- seq(1:ncol(x))
    
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    results <- foreach::foreach(i = x.index, .combine = rbind) %dopar% {
      
      fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                                paste(colnames(all.data)[x[,i]], collapse= "+")))
      nodes <- round(no.vars/2)
      NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
      
      ## Predict using NN
      predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
      if(is.null(predict.NN)){
        RMSE.NN <- NA
        r2.NN <- NA
      } else {
        predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
        # Calculate Root Mean Square Error (RMSE)
        RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
        # Calculate R-sq
        r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
      }
      data.frame("Fmla" = as.character(fmla[3]),
                 "RMSE" = RMSE.NN,
                 "r2" = as.numeric(r2.NN),
                 "Nodes" = nodes)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
    write.csv(results, paste0("results/PAA_1min_NN_",results[1,4],"nodes_",no.vars,"vars_",iteration,".csv"))
    
    
  }
  }
  
  
  file <- paste0("results/",list.files(path="results"))
  for(i in 1:length(file)){
    raw <- read.csv(file[i], row.names=1, stringsAsFactors=FALSE)
    if(i ==1) data <- raw
    if(i != 1)data <- rbind(data,raw)
  }
  
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    data.avg <- foreach::foreach(i = 1:length(unique(data$Fmla)), .combine = rbind) %dopar% {
    result <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    data.frame("Fmla" = unique(data$Fmla)[i],
               "R2" = result)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
      # Rank r2 and RMSE to find best model
  best.r2 <- which(data.avg[,2] == max(data.avg[,2]))
  best.fmla <- unique(data$Fmla)[best.r2]
  
  fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                            best.fmla))
  nodes <- round(no.vars/2)
  NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
  
  ## Predict using NN
  predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
  if(is.null(predict.NN)){
    RMSE.NN <- NA
    r2.NN <- NA
  } else {
    predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
    # Calculate Root Mean Square Error (RMSE)
    RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
    # Calculate R-sq
    r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
  }
  
}


# Data to test
# # FIX
# # test.data <- more.nsec[paste0(range(rownames(all.data))[2],"/")]
# test.data <- more.nsec[paste0(range(index(all.data))[2],"/")]
# test.data <- test.data[-unique(as.numeric(unlist(apply(test.data,2,function(x) which(is.na(x)))))),]
# # test.data <- mergeData(list.x=list(more.vis.all[paste0(range(rownames(all.data))[2],"/",range(index(test.data))[2])], 
# #                                    test.data))
# test.data <- mergeData(list.x=list(test.data, more.vis.all[paste0(range(index(test.data))[1],"/",range(index(test.data))[2])]))
# C1 <- Neural network prediction
C1 <- neuralnet::compute(NN, testing.NN)
# t1 <- HRT to the "1-min sample point"
t1 <- 130.5293*testing.data$N..Basin.Outfall^-0.9495
```

# The online Carbovis analyzer 
```{r}
C2 <- testing.data$PAA.Upstream.Residual
t2 <- 130.5293*testing.data$N..Basin.Outfall^-0.9495 + 10
```

# NN Models were built to predict PAA at the 1/2 basin sample point to predict C3
```{r}
# Build NN for 1/2 basin
all.data <- oct.paa
r <- paste0(range(index(all.data))[1],"/",range(index(all.data))[2])
all.data <- mergeData(list.x = list(all.data,vis.data[r]))
all.data <- mergeData(list.x = list(all.data, nsec.online[r,22:28]), average = FALSE)
all.data <- na.omit(all.data)
remove.cols.names <- c("Initial.PAA.Demand.or.Decay", "DPAA.Samples","Sample.Time..1_min.","Detention.Time","PAA.Pump.Total.Flow", "PAA.Set.Point.Dose.Algorithm", "...10" , "Volume.to.1.min..Sample" , "Time.to.1.min..Sample", "Total.Basin.Volume", "DT.of.1.2.Basin", "SPBased.Disinfection.CT",     "CalcBased.Disinfection.CT", "CODto..V.", "CODds..V.", "TSS..V.", "UVT..V.", "UVT..V.", "SACto..V.")
remove.cols <- sapply(remove.cols.names, function(x) which(colnames(all.data) == x))
all.data <- all.data[,-remove.cols]

Optimize <- FALSE
if(Optimize){
  predict.col.name <- c("PAA...1.min..Sample", "PAA...1.2.Basin.Sampling")

percent.train = 0.8
training.index = NULL
all.data <- all.data[,which(colnames(all.data) != predict.col.name[1])]
predict.col.name <- predict.col.name[2]
iterations <- 10 

# NNopt <- function(all.data, predict.col.name, percent.train = 0.8, training.index=NULL) {
  library(doSNOW)
  library(parallel)
  library(neuralnet)

  ## Prep data
  predict.column <- which(colnames(all.data) == predict.col.name)
  all.data <- na.omit(data.frame(all.data))
  
  ## Scale data using min-max method
  max <- apply(all.data, 2, max)
  min <- apply(all.data, 2, min)
  scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))
  
  for(iteration in 1:iterations) {
  ## Create training and test datasets
  # if(is.null(training.index)) {
    set.seed(Sys.time())
    training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
  # }
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  
  ## Find optimum number of variables
  n <- which(colnames(all.data) != predict.col.name)
  x.combos <- c(3,4,5,6,7,8,9)
  for(no.vars in x.combos) {

    x <- data.frame(combn(n,no.vars))
    x.index <- seq(1:ncol(x))
    
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    results <- foreach::foreach(i = x.index, .combine = rbind) %dopar% {
      
      fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                                paste(colnames(all.data)[x[,i]], collapse= "+")))
      nodes <- round(no.vars/2)
      NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
      
      ## Predict using NN
      predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
      if(is.null(predict.NN)){
        RMSE.NN <- NA
        r2.NN <- NA
      } else {
        predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
        # Calculate Root Mean Square Error (RMSE)
        RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
        # Calculate R-sq
        r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
      }
      data.frame("Fmla" = as.character(fmla[3]),
                 "RMSE" = RMSE.NN,
                 "r2" = as.numeric(r2.NN),
                 "Nodes" = nodes)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
    write.csv(results, paste0("results/PAA_1-2basin_NN_",results[1,4],"nodes_",no.vars,"vars_",iteration,".csv"))
    
    
  }
  }
  
  
  file <- paste0("results/",list.files(path="results"))
  file <- file[grep("PAA_1-2basin",file)]

  for(i in 1:length(file)){
    raw <- read.csv(file[i], row.names=1, stringsAsFactors=FALSE)
    if(i ==1) data <- raw
    if(i != 1)data <- rbind(data,raw)
  }
  
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    data.avg <- foreach::foreach(i = 1:length(unique(data$Fmla)), .combine = rbind) %dopar% {
    result <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    data.frame("Fmla" = unique(data$Fmla)[i],
               "R2" = result)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
      # Rank r2 and RMSE to find best model
  best.r2 <- which(data.avg[,2] == max(data.avg[,2]))
  best.fmla <- unique(data$Fmla)[best.r2]
  print.this <- data.avg[order(-data.avg[,2]),]
  write.csv(print.this,"some_results.csv")
  fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                            best.fmla))
  nodes <- round(no.vars/2)
  NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
  
  ## Predict using NN
  predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
  if(is.null(predict.NN)){
    RMSE.NN <- NA
    r2.NN <- NA
  } else {
    predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
    # Calculate Root Mean Square Error (RMSE)
    RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
    # Calculate R-sq
    r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
  }
  
}




# C3 <- 
t3 <- 1477.2*test.data$Dis.North.Flow^-.958
```





# Scrap
```{r}
  # fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                            # paste(colnames(all.data)[-predict.column], collapse= "+")))
  

  
  
  # ## Find optimum number of hidden nodes
  # results <- foreach::foreach(i = 1:(ncol(all.data)-1), .combine = rbind) %dopar% {
  #   
  #   hidden.nodes <- i
  #   NN <- neuralnet::neuralnet(fmla, training.NN, hidden = hidden.nodes, linear.output = FALSE)
  #   
  #   ## Predict using NN
  #   predict.NN <-  neuralnet::compute(NN, testing.NN[,-predict.column])
  #   predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
  #   
  #   # Calculate Root Mean Square Error (RMSE)
  #   RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
  #   # Calculate R-sq
  #   source("GitHub\\functions\\r-sq.R")
  #   r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
  #   
  #   data.frame("Hidden Nodes" = hidden.nodes,
  #              "RMSE" = RMSE.NN,
  #              "r2" = r2.NN)
  # }

  # data.top5 <- subset(data, r2 > quantile(r2, prob = 1 - 1/100))
  # data.vars <- colnames(all.data)
  # data.vars <- sapply(data.vars, function(x) grepl(x, data.top5[,1]))
  # data.vars.count <- apply(data.vars,2,function(x)length(which(x)))
  # data.nodes.count <- sapply(unique(data.top5$Nodes), function(x) length(which(data.top5$Nodes == x)))
  # names(data.nodes.count) <- unique(data.top5$Nodes)
  
```

