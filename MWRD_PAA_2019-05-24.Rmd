---
title: "MWRD_PAA_2019-05-24"
---
# Import data and functions
```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
rm(list = ls())
# setwd("..")
# source("functions/r-sq.R")
r.sq <- function(raw,fit) {
  # y <- raw
  # y_bar <- mean(raw)
  # y_hat <- fit
  # r_sq <- sum((y_hat-y_bar)^2) / sum((y-y_bar)^2)
  r_sq <- cor(raw,fit)^2
  return(r_sq)
}
# setwd("GitHub")
# setwd("~/KNewhart/MWRD")
library(xts)
mergeData <- function(list.x, sort.by = 1, average = FALSE) {
  library(xts)
  
  all.data <- do.call(merge, list.x)
  all.data.index <- which(!is.na(all.data[,sort.by]))
  for(i in 1:(length(all.data.index)-1)) {
    row.start <- all.data.index[i]
    row.stop <- all.data.index[i+1]
    if((row.stop-row.start) == 1) {
      next
    }
    if(!average) data.locf <- na.locf(all.data[(row.start+1):row.stop,])
    if(average) {
      data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
      rownames(data.avg) <- as.character(index(all.data)[row.stop])
      
    }
    if (!exists("new.data")) {
      if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
      if(average) new.data <- data.avg
    }
    if (exists("new.data")) {
      if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
      if(average) new.data <- rbind(new.data, data.avg)
    }
  }
  new.data <- na.omit(new.data)
  na.fix <- which(!is.na(as.POSIXct(rownames(new.data), format = "%Y-%m-%d %H:%M:%S")))
  new.data.xts <- xts(new.data[na.fix,], order.by = as.POSIXct(rownames(new.data)[na.fix], format = "%Y-%m-%d %H:%M:%S"))
  
  return(new.data.xts)
}


##### PAA Process Data - Grab #####
delta <- intToUtf8(0x0394)
# Daily data
process.data <- readxl::read_excel("data/PAA PROFILE DATA_08-12-18.xlsx", 
                                   sheet = "Process Data", skip = 1)
process.data <- process.data[-1,]
n.paa.grab <- xts::xts(apply(process.data[,c(12:17,19:27)], 2, function(x) as.numeric(x)), order.by =  as.POSIXct(as.data.frame(process.data[,18])[,1], format = "%Y-%m-%d %H:%M:%S"))
colnames(n.paa.grab) <- c("PAA Dosing Pump Total Flow (gpm)", #1 
                          "PAA Dose (mg/L)", #2
                          "PAA Setpoint (mg/L)", #3 
                          "Upstream  Residual (mg/L)", #4 
                          # paste0(delta,"PAA (mg/L)"),	#5
                          "deltaPAA (mg/L)", #5
                          "Pre-Disinfection E. coli (MPN/100 mL)",  #6
                          "Effluent Discharge (MGD)", #7
                          "Contact Tank Volume (MG)", #8
                          "Detention Time (min)", #9
                          "Time to Upstream Sample Point (min)", #10
                          "Log Removal (N0/N)", #11
                          "Effluent E. coli (MPN/100 mL)", #12
                          "CT (mg/L*min)", #13
                          "CuT (mg/L*min)", #14
                          "Ambient Temperature")#15
colnames(n.paa.grab) <- stringr::str_replace_all(colnames(n.paa.grab), c(" " = ".", "/" = "." , "-" = "","[(]" = "", "[)]" = "", "[*]"="."))
rm(process.data)


##### October PAA Process Data - Grab #####
oct.paa <- readxl::read_excel("data/PAA PROFILE DATA_08-12-18.xlsx", 
                              sheet = "Oct 2 to 15, 2018", range = "A1:V170")[-1,]
n.datetime <- which(colnames(oct.paa) == "Date and Time")
oct.paa.index <- oct.paa[,n.datetime]
oct.paa <- sapply(oct.paa[,-n.datetime], function(x) as.numeric(x))
colnames(oct.paa) <- stringr::str_replace_all(colnames(oct.paa), c(" " = "." , "-" = "" ))
oct.paa <- xts(oct.paa, order.by = oct.paa.index[[1]])

###### North Carbovis Data #####
vis.data <- readxl::read_excel("data/NNE Carbovis Data 2018.xlsx",
                               sheet = "Inst DL Data", col_types = c("date",
                                                                     "text", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "numeric", "skip", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "numeric", "skip",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip", "skip", "skip", "numeric",
                                                                     "skip"), skip = 6)
vis.data <- vis.data[which(vis.data[,2] == "Valid"),-2]
colnames(vis.data) <- c("Time", "CODto (mg/L)", "CODto (V)",
                        "TSS (mg/L)", "TSS (V)",
                        "UVT (%)", "UVT (V)",
                        "CODds (mg/L)", "CODds (V)",
                        "SACto (1/m)", "SACto (V)")
vis.data <- xts(vis.data[,-1], order.by = as.POSIXct(as.data.frame(vis.data[,1])[,1], format = "%Y-%m-%d %H-%M-%S"))


##### North Secondary - Online #####
## North secondary online
nsec.online <- as.data.frame(suppressWarnings(readxl::read_excel("data/North Secondary and Disinfection Process Data_2018.xlsx", sheet = "NSEC Online Data", col_names = FALSE,
                                                                 col_types = c("date", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric", 
                                                                               "numeric", "numeric", "numeric"), 
                                                                 skip = 4)))
nsec.online <- xts(nsec.online[,-1], order.by = nsec.online[,1])
colnames(nsec.online) <- c("NSEC Influent Flow", "NSEC Influent Temp","NSEC Influent NH3","NSEC Influent TSS","NSEC Influent COD",
                           "NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3",
                           "GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow",
                           "AB-10 Influent Flow","AB-10 A-Pass Temp","AB-10 A-Pass pH","AB-10 A-Pass DO","AB-10 A-Pass NH3","AB-10 A-Pass NO3","AB-10 B-Pass DO","AB-10 C-Pass pH	AB-10","C-Pass DO","AB-10 C-Pass NH3","AB-10 C-Pass NO3","AB-10 MLSS","AB-10 MLR Flow","Quad 4 RAS Flow","Quad 4 Basins in Service","AB-10 RAS Flow","NSEC Aerobic SRT",
                           "NSEC Effluent NH3","NSEC Effluent NO3","NSEC Effluent OP","NSEC Effluent TSS","NSEC Effluent NO5","NSEC Effluent Flow")


# nsec.online <- nsec.online["2018-11-04/2018-12-01"]
cols2remove <- c("NSEC CaRRB-1 Centrate Flow","NSEC CaRRB-1 NH3","NSEC CaRRB-3 Centrate Flow","NSEC CaRRB-3 NH3","GTE Flow","GTE to SSEC Flow","GTE to NSEC Flow")

nsec.online <- nsec.online[,-sapply(cols2remove, function(x) which(colnames(nsec.online) == x))]

more.nsec <- readxl::read_excel("data/PAA-Ecoli.xlsx", 
                                sheet = "Sheet1", col_types = c("date", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric", "numeric", 
                                                                "numeric", "numeric"))
more.nsec <- more.nsec[which(!is.na(more.nsec[,1])),]
more.nsec <- xts(more.nsec[,-1], order.by = more.nsec[,1][[1]])
colnames(more.nsec) <- c("PAA Upstream Residual", "PAA Total Flow", "Dis North Flow", 
                         "Temperature NSEC Inf", "ASRT", "NSEC Effluent NH3",
                         "NSEC Effluent NO3","NSEC Effluent OP","NSEC Effluent TSS",
                         "NSEC Effluent NO5","NSEC Effluent Flow")

library(readr)
more.vis <- read_csv("data/Carbovis_May2019.csv", 
    col_types = cols(Time = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_1 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_2 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_3 = col_datetime(format = "%m/%d/%Y %H:%M"), 
        Time_4 = col_datetime(format = "%m/%d/%Y %H:%M")))
r <- range(data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1])
# more.vis.all <- mergeData(list.x=list(xts(na.omit(more.vis[,2]), order.by = data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1]),
                                  # xts(na.omit(more.vis[,4]), order.by = data.frame(more.vis[-which(is.na(more.vis[,4])),3][1])[,1])[paste0(r[1],"/",r[2])]))
more.vis.all <- merge(xts(na.omit(more.vis[,2]), order.by = data.frame(more.vis[-which(is.na(more.vis[,2])),1][1])[,1]),
                                  xts(na.omit(more.vis[,4]), order.by = data.frame(more.vis[-which(is.na(more.vis[,4])),3][1])[,1])[paste0(r[1],"/",r[2])])
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,6]), order.by = data.frame(more.vis[-which(is.na(more.vis[,6])),5][1])[,1]))
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,8]), order.by = data.frame(more.vis[-which(is.na(more.vis[,8])),7][1])[,1]))
more.vis.all <- merge(more.vis.all, 
                      xts(na.omit(more.vis[,10]), order.by = data.frame(more.vis[-which(is.na(more.vis[,10])),9][1])[,1]))
more.vis.all <- more.vis.all[!is.na(more.vis.all[,1]),]

all.data <- oct.paa
r <- paste0(range(index(all.data))[1],"/",range(index(all.data))[2])
all.data <- mergeData(list.x = list(all.data,vis.data[r]))
all.data <- mergeData(list.x = list(all.data, nsec.online[r,22:28]), average = FALSE)
all.data <- na.omit(all.data)
remove.cols.names <- c("Initial.PAA.Demand.or.Decay", "DPAA.Samples","Sample.Time..1_min.","Detention.Time","PAA.Pump.Total.Flow", "PAA.Set.Point.Dose.Algorithm", "...10" , "Volume.to.1.min..Sample" , "Time.to.1.min..Sample", "Total.Basin.Volume", "DT.of.1.2.Basin", "SPBased.Disinfection.CT",     "CalcBased.Disinfection.CT", "CODto..V.", "CODds..V.", "TSS..V.", "UVT..V.", "UVT..V.", "SACto..V.")
remove.cols <- sapply(remove.cols.names, function(x) which(colnames(all.data) == x))
all.data <- all.data[,-remove.cols]


```

# NN Models were built to predict PAA at the "1-min" and "1/2 basin" sample point to predict C1 and C2
```{r include=FALSE}
Optimize <- FALSE
if(Optimize){
  predict.col.name <- c("PAA...1.min..Sample", "PAA...1.2.Basin.Sampling")

percent.train = 0.8
training.index = NULL
all.data <- all.data[,which(colnames(all.data) != predict.col.name[2])]
predict.col.name <- predict.col.name[1]
iterations <- 10 

# NNopt <- function(all.data, predict.col.name, percent.train = 0.8, training.index=NULL) {
  library(doSNOW)
  library(parallel)
  library(neuralnet)

  ## Prep data
  predict.column <- which(colnames(all.data) == predict.col.name)
  all.data <- na.omit(data.frame(all.data))
  
  ## Scale data using min-max method
  max <- apply(all.data, 2, max)
  min <- apply(all.data, 2, min)
  scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))
  
  for(iteration in 1:iterations) {
  ## Create training and test datasets
  # if(is.null(training.index)) {
    set.seed(Sys.time())
    training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
  # }
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  
  ## Find optimum number of variables
  n <- which(colnames(all.data) != predict.col.name)
  x.combos <- c(3,4,5,6,7,8,9)
  for(no.vars in x.combos) {

    x <- data.frame(combn(n,no.vars))
    x.index <- seq(1:ncol(x))
    
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    results <- foreach::foreach(i = x.index, .combine = rbind) %dopar% {
      
      fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                                paste(colnames(all.data)[x[,i]], collapse= "+")))
      nodes <- round(no.vars/2)
      NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
      
      ## Predict using NN
      predict.NN <-  tryCatch(neuralnet::compute(NN, testing.NN[,-predict.column]), error = function(e){})
      if(is.null(predict.NN)){
        RMSE.NN <- NA
        r2.NN <- NA
      } else {
        predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
        # Calculate Root Mean Square Error (RMSE)
        RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
        # Calculate R-sq
        r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
      }
      data.frame("Fmla" = as.character(fmla[3]),
                 "RMSE" = RMSE.NN,
                 "r2" = as.numeric(r2.NN),
                 "Nodes" = nodes)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
    write.csv(results, paste0("results/PAA_1min_NN_",results[1,4],"nodes_",no.vars,"vars_",iteration,".csv"))
    
    
  }
  }
  
  file <- paste0("results/",list.files(path="results"))
  for(i in 1:length(file)){
    raw <- read.csv(file[i], row.names=1, stringsAsFactors=FALSE)
    if(i ==1) data <- raw
    if(i != 1)data <- rbind(data,raw)
  }
  
  
  
    # detect cores with parallel() package
    nCores <- detectCores(logical = FALSE)
    # detect threads with parallel()
    nThreads<- detectCores(logical = TRUE)
    
    # Create doSNOW compute cluster
    cluster = makeCluster(nThreads, type = "SOCK")
    class(cluster);
    
    # register the cluster
    registerDoSNOW(cluster)
    
    data.avg <- foreach::foreach(i = 1:length(unique(data$Fmla)), .combine = rbind) %dopar% {
  # for(i in 1:length(unique(data$Fmla))){
  # for(i in 4594:length(unique(data$Fmla))){
    # if(i == 1) data.avg <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    # if(i != 1) data.avg <- c(data.avg, mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])]))
    result <- mean(data$r2[which(data$Fmla == unique(data$Fmla)[i])])
    data.frame("Fmla" = unique(data$Fmla)[i],
               "R2" = result)
    }
    
    # stop cluster and remove clients
    stopCluster(cluster)
    
    # insert serial backend, otherwise error in repetetive tasks
    registerDoSEQ()
    
    # clean up a bit.
    invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
    
}
  

    
  # data.avg <- sapply(unique(data$Fmla), function(x) mean(data$r2[which(data$Fmla == x)]))
  # data.avg <- cbind(unique(data$Fmla), sapply(data.index, function(x) mean(data$Fmla[x])))


all.data <- oct.paa
r <- paste0(range(index(all.data))[1],"/",range(index(all.data))[2])
all.data <- mergeData(list.x = list(all.data,vis.data[r]))
all.data <- mergeData(list.x = list(all.data, nsec.online[r,22:28]), average = FALSE)
all.data <- na.omit(all.data)
remove.cols.names <- c("Initial.PAA.Demand.or.Decay", "DPAA.Samples","Sample.Time..1_min.","Detention.Time","PAA.Pump.Total.Flow", "PAA.Set.Point.Dose.Algorithm", "...10" , "Volume.to.1.min..Sample" , "Time.to.1.min..Sample", "Total.Basin.Volume", "DT.of.1.2.Basin", "SPBased.Disinfection.CT",     "CalcBased.Disinfection.CT", "CODto..V.", "CODds..V.", "TSS..V.", "UVT..V.", "UVT..V.", "SACto..V.")
remove.cols <- sapply(remove.cols.names, function(x) which(colnames(all.data) == x))
all.data <- all.data[,-remove.cols]

for(obs in c(10, 30,35,40,45,50,55,60,65,70,90,110)) {

predict.col.name <- c("PAA...1.min..Sample")
## Prep data
predict.column <- which(colnames(all.data) == predict.col.name)
all.data <- na.omit(data.frame(all.data))
## Scale data using min-max method
max <- apply(all.data, 2, max)
min <- apply(all.data, 2, min)
scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))


percent.train = obs/nrow(all.data)
# set.seed(Sys.time())
# training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  best.fmla <- as.formula("PAA...1.min..Sample ~ Pump.Flow.Based.PAA.Dose + Temp..of.NSEC.Main.Ch. + Temp..of.the.Atmos. + Secondary.Effluent.Flow + NSEC.Effluent.OP + NSEC.Effluent.NO5 + NSEC.Effluent.Flow")
  fmla <- best.fmla
  no.vars <- 9
  nodes <- round(no.vars/2)
  NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
  predict.NN <-  neuralnet::compute(NN, testing.NN[i,])
  predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
# 
#   
#   C <- training.data$PAA...1.min..Sample
#   t <- 130.5293*training.data$N..Basin.Outfall^-0.9495
#   exponential.model <- lm(log(C) ~ t)
#   k <- -1*exponential.model$coefficients[2]
#   C0 <- exp(exponential.model$coefficients[1])
#   Cp <- as.numeric(C0*exp(-k*130.5293*testing.data$N..Basin.Outfall[i]^-0.9495))
#   
  t <- 130.5293*testing.data$N..Basin.Outfall[i]^-0.9495
  

  if(i == 1) results <- c(testing.data[i,predict.column],
                          predict.NN,t)
  if(i > 1) results <- rbind(results, c(testing.data[i,predict.column],
                          predict.NN,t))
}
rownames(results) <- rownames(all.data)[-c(1:(nrow(all.data)*(percent.train)))]
colnames(results) <- c("Actual", "NN C", "HRT")
# View(results)
results.metrics <- c((sum((results[,1] - results[,2])^2) / nrow(results)) ^ 0.5, cor(results[,1],results[,2])^2)




# # Data to test
# # test.data <- more.nsec[paste0(range(rownames(all.data))[2],"/")]
# test.data <- more.nsec[paste0(range(index(all.data))[2],"/")]
# test.data <- test.data[-unique(as.numeric(unlist(apply(test.data,2,function(x) which(is.na(x)))))),]
# # test.data <- mergeData(list.x=list(more.vis.all[paste0(range(rownames(all.data))[2],"/",range(index(test.data))[2])], 
# #                                    test.data))
# test.data <- mergeData(list.x=list(more.vis.all[paste0(range(index(all.data))[2],"/",range(index(test.data))[2])], 
#                                    test.data))
# 
# 
# 

# C1 <- Neural network prediction
# C1 <- neuralnet::compute(NN, data.frame(test.data))
C1 <- results
# t1 <- HRT to the "1-min sample point"
t1 <- 130.5293*all.data$N..Basin.Outfall[-c(1:(nrow(all.data)*(1-percent.train)))]^-0.9495




# Build NN for 1/2 basin


predict.col.name <- c("PAA...1.2.Basin.Sampling")
## Prep data
predict.column <- which(colnames(all.data) == predict.col.name)
all.data <- na.omit(data.frame(all.data))
## Scale data using min-max method
max <- apply(all.data, 2, max)
min <- apply(all.data, 2, min)
scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))

# percent.train = 0.5
# set.seed(Sys.time())
# training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  training.NN <- scaled.data[training.index,]
  testing.NN <- scaled.data[-training.index,]
  best.fmla <- as.formula("PAA...1.2.Basin.Sampling ~ N..Eff..TSS.Conc. + N..Basin.Outfall + TSS..mg.L. + UVT.... + NSEC.Aerobic.SRT + NSEC.Effluent.NO3 + NSEC.Effluent.NO5")
  fmla <- best.fmla
  no.vars <- 9
  nodes <- round(no.vars/2)
  NN <- neuralnet::neuralnet(fmla, training.NN, hidden = nodes, linear.output = FALSE)
  predict.NN <-  neuralnet::compute(NN, testing.NN[i,])
  predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])

  
  # C <- training.data$PAA...1.2.Basin.Sampling
  t <- 1477.2*testing.data$N..Basin.Outfall[i]^-0.958

  
  
  

  if(i == 1) results <- c(testing.data[i,predict.column],
                          predict.NN,t)
  if(i > 1) results <- rbind(results, c(testing.data[i,predict.column],
                          predict.NN,t))
}
rownames(results) <- rownames(all.data)[-c(1:(nrow(all.data)*(percent.train)))]
colnames(results) <- c("Actual", "NN C", "HRT")
# View(results)

results.metrics <- c((sum((results[,1] - results[,2])^2) / nrow(results)) ^ 0.5, cor(results[,1],results[,2])^2)




# # Data to test
# # test.data <- more.nsec[paste0(range(rownames(all.data))[2],"/")]
# test.data <- more.nsec[paste0(range(index(all.data))[2],"/")]
# test.data <- test.data[-unique(as.numeric(unlist(apply(test.data,2,function(x) which(is.na(x)))))),]
# # test.data <- mergeData(list.x=list(more.vis.all[paste0(range(rownames(all.data))[2],"/",range(index(test.data))[2])], 
# #                                    test.data))
# test.data <- mergeData(list.x=list(more.vis.all[paste0(range(index(all.data))[2],"/",range(index(test.data))[2])], 
#                                    test.data))
# 
# 
# 

# C1 <- Neural network prediction
# C1 <- neuralnet::compute(NN, data.frame(test.data))
C3 <- results
t3 <- 1477.2*all.data$N..Basin.Outfall[-c(1:(nrow(all.data)*(1-percent.train)))]^-.958








# Let's plot C1 and C3
# source("ct.R")
# actualCt <- calcCt(C1=C1[,1], t1 = t1, C2 = C3[,1], t2=t3)
# predictedCt <- calcCt(C1=C1[,2], t1 = t1, C2 = C3[,2], t2=t3)
for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  C <- c(C1[i,2], C3[i,2]) # NN predicted C
  t <- c(C1[i,3], C3[i,3]) # HRT
  exponential.model <- lm(log(C) ~ t) # Exponential model fit of NN predictions
  k <- -1*exponential.model$coefficients[2]
  C0 <- exp(exponential.model$coefficients[1])
  t_f <- 2775*testing.data$N..Basin.Outfall[i]^-.959
  Ct <- C0/k-C0/k*exp(-k*t_f) # Ct under exponential curve to end of basin
  
  if(i == 1) results <- Ct
  if(i > 1) results <- rbind(results, Ct)
  # 
  # C <- c(training.data$PAA...1.min..Sample, training.data$PAA...1.2.Basin.Sampling)
  # t <- c(130.5293*training.data$N..Basin.Outfall^-0.9495, 
  #        1477.2*training.data$N..Basin.Outfall^-.958)
  # exponential.model <- lm(log(C) ~ t)
  # k <- -1*exponential.model$coefficients[2]
  # C0 <- exp(exponential.model$coefficients[1])
  # Ct <- C0/k-C0/k*exp(-k*t) # Ct under single exponential curve
}

Ct.predicted <- results

for (i in 1:(nrow(all.data)*(1-percent.train))) {
  training.index <- seq(i,(nrow(all.data)*percent.train-1+i),by=1)
  training.data <- all.data[training.index,]
  testing.data <- all.data[-training.index,]
  C <- c(C1[i,1], C3[i,1]) # Actual C
  t <- c(C1[i,3], C3[i,3]) # HRT
  exponential.model <- lm(log(C) ~ t) # Exponential model fit of NN predictions
  k <- -1*exponential.model$coefficients[2]
  C0 <- exp(exponential.model$coefficients[1])
  t_f <- 2775*testing.data$N..Basin.Outfall[i]^-.959
  Ct <- C0/k-C0/k*exp(-k*t_f) # Ct under exponential curve to end of basin
  
  if(i == 1) results <- Ct
  if(i > 1) results <- rbind(results, Ct)
}

Ct.actual <- results
r2.NN <- cor(Ct.actual, Ct.predicted)^2


n.obs <- as.numeric(nrow(all.data) - nrow(Ct.actual))
png(paste0("Ct_",n.obs,"obs.png"), width = 600, height = 400)
plot(Ct.actual, type="l", lwd=1.5,xlab = "Sampling Event", ylab="Ct (mg/L*min)", main=paste0("Ct - trained on ",n.obs," obs"))
points(Ct.predicted, type="l", col = "purple", lwd=1.5)
legend("topleft", legend = c("Actual Ct", "Predicted Ct"), lty = 1, lwd=1.5,col=c("black", "purple"))
text(x=nrow(Ct.actual),y=min(Ct.actual), labels=paste("R2 =", as.character(round(r2.NN,2))), adj=1)
dev.off()

print(paste("For",as.character(n.obs),
  "obs, R2 =", as.character(round(r2.NN,2))))
}
```


# Scrap
```{r}
  # fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
                            # paste(colnames(all.data)[-predict.column], collapse= "+")))
  

  
  
  # ## Find optimum number of hidden nodes
  # results <- foreach::foreach(i = 1:(ncol(all.data)-1), .combine = rbind) %dopar% {
  #   
  #   hidden.nodes <- i
  #   NN <- neuralnet::neuralnet(fmla, training.NN, hidden = hidden.nodes, linear.output = FALSE)
  #   
  #   ## Predict using NN
  #   predict.NN <-  neuralnet::compute(NN, testing.NN[,-predict.column])
  #   predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
  #   
  #   # Calculate Root Mean Square Error (RMSE)
  #   RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
  #   # Calculate R-sq
  #   source("GitHub\\functions\\r-sq.R")
  #   r2.NN <- r.sq(raw=testing.data[,predict.column], fit = predict.NN)
  #   
  #   data.frame("Hidden Nodes" = hidden.nodes,
  #              "RMSE" = RMSE.NN,
  #              "r2" = r2.NN)
  # }

  # data.top5 <- subset(data, r2 > quantile(r2, prob = 1 - 1/100))
  # data.vars <- colnames(all.data)
  # data.vars <- sapply(data.vars, function(x) grepl(x, data.top5[,1]))
  # data.vars.count <- apply(data.vars,2,function(x)length(which(x)))
  # data.nodes.count <- sapply(unique(data.top5$Nodes), function(x) length(which(data.top5$Nodes == x)))
  # names(data.nodes.count) <- unique(data.top5$Nodes)
  
```

