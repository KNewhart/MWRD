"Pre-dis E.coli",
"af:\\\\APPLEPI_AF\\MWRD_Production\\Labworks Data\\012_700_1011-RWH North, Pre-PAA|ECIDX_G",
"Post-dis E.coli",
"af:\\\\APPLEPI_AF\\MWRD_Production\\Labworks Data\\2B-North Final Effluent Platform|ECIDX_G"
)
, ncol = 2, byrow = TRUE)
#
# start <- paste0(as.character(as.Date(Sys.time()) - 2), "T00:00:00Z")
# end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
# for(i in 1:nrow(pi.tags)) {
#   assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
# assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
# }
#
# # rawData <- mergeData(list(get(make.names(pi.tags[,1]))))
# for(i in 1:nrow(pi.tags)) {
#   if (i == 1) {
#     rawData <- get(make.names(pi.tags[i,1]))
#     colnames(rawData)[i] <- make.names(pi.tags[i,1])
#   } else {
#     rawData <- cbind(rawData, get(make.names(pi.tags[i,1])))
#     colnames(rawData)[i] <- make.names(pi.tags[i,1])
#   }
# }
start <- paste0(as.character(as.Date(Sys.time()) - 366), "T00:00:00Z")
end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
for(i in 5:6) {
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
# Import Carbovis
Carbovis_May2019 <- readr::read_csv("data/Carbovis_May2019.csv",
col_types = cols(Time = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_1 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_2 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_3 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_4 = col_datetime(format = "%m/%d/%Y %H:%M")))
object.list <- list()
for(i in 1:4) {
x <- which(!is.na(Carbovis_May2019[,i*2]))
assign(make.names(colnames(Carbovis_May2019)[i*2]), xts(Carbovis_May2019[x,i*2], order.by = Carbovis_May2019[x,(i*2-1)][[1]]))
object.list <-c(object.list, list(make.names(colnames(Carbovis_May2019)[i*2])))
}
all.data.ecoli <- mergeData(list(Pre.dis.E.coli, UVT))
rm(list=ls())
setwd("C:/Users/KNewhart/Documents/GitHub/MWRD")
library(readxl)
library(xts)
library(readr)
library(doSNOW)
library(parallel)
library(neuralnet)
mergeData <- function(list.x, sort.by = 1, average = FALSE) {
library(xts)
all.data <- do.call(merge, list.x)
all.data.index <- which(!is.na(all.data[,sort.by]))
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
## Find optimum number of hidden nodes
new.data <- foreach::foreach(i = 1:(length(all.data.index)-1), .combine = rbind) %dopar% {
# for(i in 1:(length(all.data.index)-1)) {
row.start <- all.data.index[i]
row.stop <- all.data.index[i+1]
if(!average) data.locf <- zoo::na.locf(all.data[(row.start+1):row.stop,])
if(average) {
data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
rownames(data.avg) <- as.character(index(all.data)[row.stop])
}
# if (i == 1) {
#   if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
#   if(average) new.data <- data.avg
# }
# if (i != 1) {
# if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
# if(average) new.data <- rbind(new.data, data.avg)
# }
if(!average) data.frame(data.locf[nrow(data.locf),])
if(average) data.avg
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster);
new.data <- na.omit(new.data)
new.data.xts <- xts(new.data, order.by = as.POSIXct(rownames(new.data), format = "%Y-%m-%d %H:%M:%S"))
return(new.data.xts)
}
# Fix timestamps - this function creates an xts object from the piWebApiService function above
fix.timestamps <- function(pi.data) {
ch.times <- pi.data[,2]
ch.times <- sub("T", " ", ch.times)
ch.times <- sub("Z", " ", ch.times)
times <- as.POSIXct(ch.times)
return(xts::xts(pi.data[,1], order.by = times))
}
NNopt <- function(all.data, predict.col.name, percent.train = 0.8, training.index=NULL) {
## Prep data
predict.column <- which(colnames(all.data) == predict.col.name)
all.data <- na.omit(data.frame(all.data))
## Scale data using min-max method
max <- apply(all.data, 2, max)
min <- apply(all.data, 2, min)
scaled.data <- as.data.frame(scale(all.data, center = min, scale = max - min))
## Create training and test datasets
if(is.null(training.index)) {
set.seed(Sys.time())
training.index <- sample(seq_len(nrow(all.data)), size=(percent.train*nrow(all.data)))
}
training.data <- all.data[training.index,]
testing.data <- all.data[-training.index,]
training.NN <- scaled.data[training.index,]
testing.NN <- scaled.data[-training.index,]
fmla <- as.formula(paste0(colnames(all.data)[predict.column],"~",
paste(colnames(all.data)[-predict.column], collapse= "+")))
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
## Find optimum number of hidden nodes
results <- foreach::foreach(i = 1:(ncol(all.data)-1), .combine = rbind) %dopar% {
hidden.nodes <- i
NN <- neuralnet::neuralnet(fmla, training.NN, hidden = hidden.nodes, linear.output = FALSE)
## Predict using NN
predict.NN <-  neuralnet::compute(NN, testing.NN[,-predict.column])
predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) - min(all.data[,predict.column]))) + min(all.data[,predict.column])
# Calculate Root Mean Square Error (RMSE)
RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
data.frame("Hidden Nodes" = hidden.nodes,
"RMSE" = RMSE.NN)
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster);
## Build best NN
hidden.nodes <- min(results[,c("RMSE")])
hidden.nodes <- which(results[,c("RMSE")] == hidden.nodes)
hidden.nodes <- as.numeric(results[hidden.nodes,"Hidden.Nodes"])
NN <- neuralnet(fmla, training.NN, hidden = hidden.nodes, linear.output = FALSE)
## Predict using NN
predict.NN <-  neuralnet::compute(NN, testing.NN[,-predict.column])
predict.NN <- (predict.NN$net.result * (max(all.data[,predict.column]) -
min(all.data[,predict.column]))) +
min(all.data[,predict.column])
# Calculate Root Mean Square Error (RMSE)
RMSE.NN <- (sum((testing.data[,predict.column] - predict.NN)^2) / nrow(testing.data)) ^ 0.5
return(list("NN" = NN,
"Actual" = testing.data[,predict.column],
"Predicted" = predict.NN,
"RMSE" = RMSE.NN))
} # End of NNopt function
# Install and load piwebapi package from Github
# install.packages("devtools")
# library(devtools)
# install_github("rbechalany/PI-Web-API-Client-R")
library(piwebapi)
# Login information
useKerberos <- TRUE
username <- "knewhart"
password <- "Lunabear2@"
validateSSL <- TRUE
debug <- TRUE
piWebApiService <- piwebapi$new("https://pivision/piwebapi", useKerberos, username, password, validateSSL, debug)
# Compile data
pi.tags <- matrix(c("DIS PAA N Upstream Residual",
"pi:\\\\applepi\\AI_K826",
"DIS N PAA Retention Time",
"pi:\\\\applepi\\PAA_N_Retention_T",
"N PAA Pump Flow",
"pi:\\\\applepi\\FY_K810",
"N PAA Total Flow",
"pi:\\\\applepi\\PAA_North_Plant_Flow",
"Pre-dis E.coli",
"af:\\\\APPLEPI_AF\\MWRD_Production\\Labworks Data\\012_700_1011-RWH North, Pre-PAA|ECIDX_G",
"Post-dis E.coli",
"af:\\\\APPLEPI_AF\\MWRD_Production\\Labworks Data\\2B-North Final Effluent Platform|ECIDX_G"
)
, ncol = 2, byrow = TRUE)
#
# start <- paste0(as.character(as.Date(Sys.time()) - 2), "T00:00:00Z")
# end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
# for(i in 1:nrow(pi.tags)) {
#   assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
# assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
# }
#
# # rawData <- mergeData(list(get(make.names(pi.tags[,1]))))
# for(i in 1:nrow(pi.tags)) {
#   if (i == 1) {
#     rawData <- get(make.names(pi.tags[i,1]))
#     colnames(rawData)[i] <- make.names(pi.tags[i,1])
#   } else {
#     rawData <- cbind(rawData, get(make.names(pi.tags[i,1])))
#     colnames(rawData)[i] <- make.names(pi.tags[i,1])
#   }
# }
# Import E.coli
start <- paste0(as.character(as.Date(Sys.time()) - 366), "T00:00:00Z")
end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
for(i in 5:6) {
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
# Import Carbovis
Carbovis_May2019 <- readr::read_csv("data/Carbovis_May2019.csv",
col_types = cols(Time = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_1 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_2 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_3 = col_datetime(format = "%m/%d/%Y %H:%M"),
Time_4 = col_datetime(format = "%m/%d/%Y %H:%M")))
object.list <- list()
for(i in 1:4) {
x <- which(!is.na(Carbovis_May2019[,i*2]))
assign(make.names(colnames(Carbovis_May2019)[i*2]), xts(Carbovis_May2019[x,i*2], order.by = Carbovis_May2019[x,(i*2-1)][[1]]))
object.list <-c(object.list, list(make.names(colnames(Carbovis_May2019)[i*2])))
}
all.data.ecoli <- mergeData(list(Pre.dis.E.coli, UVT))
mergeData <- function(list.x, sort.by = 1, average = FALSE) {
all.data <- do.call(merge, list.x)
all.data.index <- which(!is.na(all.data[,sort.by]))
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
## Find optimum number of hidden nodes
new.data <- foreach::foreach(i = 1:(length(all.data.index)-1), .combine = rbind) %dopar% {
library(xts)
# for(i in 1:(length(all.data.index)-1)) {
row.start <- all.data.index[i]
row.stop <- all.data.index[i+1]
if(!average) data.locf <- na.locf(all.data[(row.start+1):row.stop,])
if(average) {
data.avg <- t(data.frame(sapply(all.data[(row.start+1):row.stop,], function(x) mean(na.omit(x)))))
rownames(data.avg) <- as.character(index(all.data)[row.stop])
}
# if (i == 1) {
#   if(!average) new.data <- data.frame(data.locf[nrow(data.locf),])
#   if(average) new.data <- data.avg
# }
# if (i != 1) {
# if(!average) new.data <- rbind(new.data, data.frame(data.locf[nrow(data.locf),]))
# if(average) new.data <- rbind(new.data, data.avg)
# }
if(!average) data.frame(data.locf[nrow(data.locf),])
if(average) data.avg
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster);
new.data <- na.omit(new.data)
new.data.xts <- xts(new.data, order.by = as.POSIXct(rownames(new.data), format = "%Y-%m-%d %H:%M:%S"))
return(new.data.xts)
}
all.data.ecoli <- mergeData(list(Pre.dis.E.coli, UVT))
library(piwebapi)
# Install and load piwebapi package from Github
rm(list=ls())
# Install and load piwebapi package from Github
# install.packages("devtools")
# library(devtools)
# install_github("rbechalany/PI-Web-API-Client-R")
library(piwebapi)
# Login information
useKerberos <- TRUE
username <- "knewhart"
password <- "Lunabear2@"
validateSSL <- TRUE
debug <- TRUE
piWebApiService <- piwebapi$new("https://pivision/piwebapi", useKerberos, username, password, validateSSL, debug)
pi.tags <- c("NPRI 1 BOD Load (t/d)","\\APPLEPI_AF\MWRD_Production\Hite Treatment Plant\04-Primaries|NPRI 1 BOD Load")
pi.tags <- c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load")
pi.tags <- matrix("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load", nrow = 1, ncol = 2)
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
View(pi.tags)
for(i in 1:nrow(pi.tags)) {
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
## Find optimum number of hidden nodes
keeper <- foreach::foreach(j = 1:nrow(all.data), .combine = rbind) %dopar% {
library(xts)
library(piwebapi)
# for(j in 1:nrow(all.data)) {
# Pull 30 min of data prior to timestamp
start <- paste0(substr(as.character(range(index(all.data[j]))[2]-30*60),1,10), "T", substr(as.character(range(index(all.data[j]))[2]-30*60), 12, 19),"Z")
end <- paste0(substr(as.character(range(index(all.data[j]))[2]),1,10), "T", substr(as.character(range(index(all.data[j]))[2]), 12, 19),"Z")
holder <- fix.timestamps(piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
# if (j == 1) {
#   keeper <- holder
# } else {
#   # Merge with full dataset
#   keeper <- rbind(keeper, holder)
holder
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster);
# Create object
assign(make.names(pi.tags[i,1]), keeper)
}
library(readxl)
library(xts)
library(readr)
library(doSNOW)
library(parallel)
library(neuralnet)
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
for(i in 1:nrow(pi.tags)) {
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
# Create doSNOW compute cluster
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
## Find optimum number of hidden nodes
keeper <- foreach::foreach(j = 1:nrow(all.data), .combine = rbind) %dopar% {
library(xts)
library(piwebapi)
# for(j in 1:nrow(all.data)) {
# Pull 30 min of data prior to timestamp
start <- paste0(substr(as.character(range(index(all.data[j]))[2]-30*60),1,10), "T", substr(as.character(range(index(all.data[j]))[2]-30*60), 12, 19),"Z")
end <- paste0(substr(as.character(range(index(all.data[j]))[2]),1,10), "T", substr(as.character(range(index(all.data[j]))[2]), 12, 19),"Z")
holder <- fix.timestamps(piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
# if (j == 1) {
#   keeper <- holder
# } else {
#   # Merge with full dataset
#   keeper <- rbind(keeper, holder)
holder
}
# stop cluster and remove clients
stopCluster(cluster)
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster);
# Create object
assign(make.names(pi.tags[i,1]), keeper)
}
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
start <- paste0("2018-06-01", "T00:00:00Z")
end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
for(i in 1:nrow(pi.tags)) {
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
# Install and load piwebapi package from Github
# install.packages("devtools")
# library(devtools)
# install_github("rbechalany/PI-Web-API-Client-R")
library(piwebapi)
# Login information
useKerberos <- TRUE
username <- "knewhart"
password <- "Lunabear2@"
validateSSL <- TRUE
debug <- TRUE
piWebApiService <- piwebapi$new("https://pivision/piwebapi", useKerberos, username, password, validateSSL, debug)
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
start <- paste0("2018-06-01", "T00:00:00Z")
end <- paste0(as.character(as.Date(Sys.time()) - 1), "T00:00:00Z")
for(i in 1:nrow(pi.tags)) {
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
piWebApiService$data$getInterpolatedValues(path=pi.tags[1,2], startTime = start, endTime = end, interval="1d")
data <- piWebApiService$data$getInterpolatedValues(path=pi.tags[1,2], startTime = start, endTime = end, interval="1d")
View(data)
pi.tags <- rbind(pi.tags,
c("NPRI 1 BOD Load (t/d)", "pi:\\\\applepi\\Primaries_NPRI1_InfBODLoad"))
data <- piWebApiService$data$getInterpolatedValues(path=pi.tags[2,2], startTime = start, endTime = end, interval="1d")
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
pi.tags <- rbind(pi.tags,
c("NPRI 1 Flow", "pi:\\\\applepi\FY-P410B"))
pi.tags <- rbind(pi.tags,
c("NPRI 1 Flow", "pi:\\\\applepi\\FY-P410B"))
i = 2
data <- piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2]
library(readxl)
library(xts)
library(readr)
library(doSNOW)
library(parallel)
library(neuralnet)
# Fix timestamps - this function creates an xts object from the piWebApiService function above
fix.timestamps <- function(pi.data) {
ch.times <- pi.data[,2]
ch.times <- sub("T", " ", ch.times)
ch.times <- sub("Z", " ", ch.times)
times <- as.POSIXct(ch.times)
return(xts::xts(pi.data[,1], order.by = times))
}
pi.tags <- read_csv("pi-tags.csv")
i = 1
substr(pi.tags[i,2],1,12)
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF")
)
(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF")
pi.tatgs[1,2]
pi.tags[1,2]
View(pi.tags)
pi.tags <- read_excel("pi-tags.xls")
pi.tags[1,2]
pi.tags <- matrix(c("NPRI 1 BOD Load (t/d)","af:\\\\APPLEPI_AF\\MWRD_Production\\Hite Treatment Plant\\04-Primaries|NPRI 1 BOD Load"), nrow = 1, ncol = 2)
pi.tags <- rbind(pi.tags,
c("NPRI 1 Flow", "pi:\\\\applepi\\FY-P410B"))
(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF")
i = 2
(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF")
pi.tags <- rbind(pi.tags,
c("NPRI 1 Flow", "\\\\applepi\\FY-P410B"))
i = 3
(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF")
(substr(pi.tags[i,2],1,11) == "\\\\applepi")
(substr(pi.tags[i,2],1,11))
(substr(pi.tags[i,2],1,10))
(substr(pi.tags[i,2],1,8))
(substr(pi.tags[i,2],1,9))
pi.tags <- read_excel("pi-tags.xls")
for(i in 1:nrow(pi.tags)) {
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF") pi.tags[i,2] <- paste0("af:", pi.tags[i,2])
if(substr(pi.tags[i,2],1,9) == "\\\\applepi") pi.tags[i,2] <- paste0("pi:", pi.tags[i,2])
}
pi.tags <- read_excel("pi-tags.xls")
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF") pi.tags[i,2] <- paste0("af:", pi.tags[i,2])
if(substr(pi.tags[i,2],1,9) == "\\\\applepi") pi.tags[i,2] <- paste0("pi:", pi.tags[i,2])
}
pi.tags <- read_excel("pi-tags.xls")
# Fix tags
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF") pi.tags[i,2] <- paste0("af:", pi.tags[i,2])
if(substr(pi.tags[i,2],1,9) == "\\\\applepi") pi.tags[i,2] <- paste0("pi:", pi.tags[i,2])
}
# Load data
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
i = 1
make.names(pi.tags[i,1])
pi.tags[i,2]
piWebApiService$data$getRecordedValues(path=pi.tags[i,2], startTime = start, endTime = end)
str(pi.tags[i,2])
str(unlist(pi.tags[i,2]))
pi.tags <- read_excel("pi-tags.xls")
# Fix tags
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF") pi.tags[i,2] <- paste0("af:", pi.tags[i,2])
if(substr(pi.tags[i,2],1,9) == "\\\\applepi") pi.tags[i,2] <- paste0("pi:", pi.tags[i,2])
}
# Load data
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=unlist(pi.tags[i,2]), startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
}
pi.tags <- read_excel("pi-tags.xls")
# Fix tags
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
if(substr(pi.tags[i,2],1,12) == "\\\\APPLEPI_AF") pi.tags[i,2] <- paste0("af:", pi.tags[i,2])
if(substr(pi.tags[i,2],1,9) == "\\\\applepi") pi.tags[i,2] <- paste0("pi:", pi.tags[i,2])
}
# Load data
new.objects <- list()
for(i in 1:nrow(pi.tags)) {
if(is.na(pi.tags[i,2])) next
assign(make.names(pi.tags[i,1]), piWebApiService$data$getRecordedValues(path=unlist(pi.tags[i,2]), startTime = start, endTime = end)[,1:2])
assign(make.names(pi.tags[i,1]), fix.timestamps(get(make.names(pi.tags[i,1]))))
new.objects <- c(new.objects, list(make.names(pi.tags[i,1])))
}
